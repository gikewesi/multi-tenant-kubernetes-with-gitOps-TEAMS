# Multi-Tenant Kubernetes Platform with GitOps & Policy Governance

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Kubernetes](https://img.shields.io/badge/Kubernetes-1.28-blue.svg)](https://kubernetes.io/)

## ğŸ¯ What This Project Really Solves

I built this platform to answer one core question:

**â€œHow do you let multiple teams share a single Kubernetes cluster without breaking each other or lowering security?â€**

In real environments Iâ€™ve worked in, I kept seeing the same problems:

* Teams deploy into each otherâ€™s namespaces because guardrails donâ€™t exist
* One buggy pod eats all cluster CPU and starves everyone else
* Developers accidentally bypass security controls
* Network access between namespaces is wide open
* Compliance teams spend hours manually checking YAML
* Monitoring teams have no tenant boundaries
* GitOps setups donâ€™t actually enforce isolationâ€¦ they just deploy things

This project demonstrates how to fix all of that **by design**, not through manual rules or constant human policing.

The entire platform is built around one principle:

### âœ… **Strong tenant isolation inside a single shared Kubernetes cluster**

Everything else (GitOps, policies, monitoring) supports that goal.

---

# Multi-Tenant Isolation Diagram

```
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚                Argo CD                   â”‚
                                   â”‚         (control plane in argocd)        â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                   â”‚
                    AppProjects                     â”‚                    CI/CD IdP
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Team A  â”‚  Team B  â”‚ Team C  â”‚         â”‚        â”‚  OIDC groups + tokens     â”‚
          â”‚  (Apps)  â”‚ (Policy)  â”‚ (Obs)  â”‚         â”‚        â”‚  ci-bot per AppProject    â”‚
          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚         â”‚          â”‚              â”‚                       â”‚
   sourceRepos, â”‚         â”‚          â”‚              â”‚     Argo RBAC roles   â”‚
 destinations,  â”‚         â”‚          â”‚              â”‚  (per-project, scoped â”‚
  kind allow/deny         â”‚          â”‚              â”‚    to each tenant)    â”‚
                â”‚         â”‚          â”‚              â”‚                       â”‚
                â–¼         â–¼          â–¼              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚ Team A App â”‚ â”‚ Policies â”‚ â”‚ Observab. â”‚  â”‚
        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â”‚
             â”‚              â”‚              â”‚        â”‚
             â–¼              â–¼              â–¼        â–¼

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  Kubernetes Cluster  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                                                                â•‘
â•‘  Namespaces per tenant:                                                                                        â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â•‘
â•‘  â”‚  team-alpha     â”‚   â”‚  governance      â”‚   â”‚  monitoring       â”‚                                            â•‘
â•‘  â”‚  (Team A apps)  â”‚   â”‚  (Team B policy) â”‚   â”‚  (Team C stack)   â”‚                                            â•‘
â•‘  â””â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â•‘
â•‘    â”‚                     â”‚                      â”‚                                                         PSA  â•‘
â•‘    â”‚   RBAC (namespace)  â”‚   RBAC (cluster+ns)  â”‚   RBAC (ns)                                             +    â•‘
â•‘    â”‚   Roles/Bindings    â”‚   for policy engine  â”‚   read across ns (metrics)                            Kyverno â•‘
â•‘    â–¼                     â–¼                      â–¼                                                               â•‘
â•‘  Workloads A         Kyverno/Gatekeeper     Prometheus, Grafana, Loki                                          â•‘
â•‘                                                                                                                â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Network Isolation (Calico/Cilium NetworkPolicies) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â•‘
â•‘  â€¢ Default deny all cross-namespace traffic                                                                     â•‘
â•‘  â€¢ Allow only app-to-approved-services (egress)                                                                 â•‘
â•‘  â€¢ Allow Team C scrape endpoints via label-based allow list                                                     â•‘
â•‘                                                                                                                â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Resource Isolation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                         â•‘
â•‘  â€¢ LimitRanges per namespace (cpu/mem requests and limits)                                                      â•‘
â•‘  â€¢ ResourceQuota per namespace (pods, PVCs, ConfigMaps, total cpu/mem)                                          â•‘
â•‘                                                                                                                â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Pod Security â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â•‘
â•‘  â€¢ PSA: baseline or restricted at namespace level                                                               â•‘
â•‘  â€¢ Kyverno rules: no privileged, no hostPath, runAsNonRoot, drop caps, read-only rootfs                         â•‘
â•‘                                                                                                                â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ingress/Egress Controls â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                    â•‘
â•‘  â€¢ Central Ingress controller with per-ns IngressClass                                                          â•‘
â•‘  â€¢ Egress via egress-gateway or NAT per team if required                                                        â•‘
â•‘                                                                                                                â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Secret Management â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                         â•‘
â•‘  â€¢ External Secrets/CSI Driver bound per namespace                                                              â•‘
â•‘  â€¢ KMS keys scoped per tenant where possible                                                                    â•‘
â•‘                                                                                                                â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Node Isolation (optional) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                    â•‘
â•‘  â€¢ Dedicated node pools with taints/tolerations per team or workload class                                      â•‘
â•‘  â€¢ Separate storage classes if needed                                                                           â•‘
â•‘                                                                                                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

The architecture is intentionally simple:
**each team gets its own namespace + its own Argo AppProject + its own policy boundaries**.

This gives every team freedom to deploy safely without impacting others.

---

# âš™ï¸ How Isolation Is Enforced (End to End)

This platform uses **6 layers of isolation**, because namespace-only â€œmulti-tenancyâ€ is not real isolation.

### âœ… 1. Namespace Isolation

Every team works in their own isolated namespace:

* Limits blast radius
* Separates workloads
* Supports clear RBAC boundaries

### âœ… 2. Argo CD AppProjects (GitOps Boundaries)

Each team has a dedicated AppProject that defines:

* Which repos they can deploy from
* Which namespaces they can target
* Which Kubernetes kinds they are allowed to create
* Their deployment roles (ci-bot, read-only, admin)

This prevents â€œaccidental cross-team deployments.â€

### âœ… 3. NetworkPolicies

Default deny across namespaces, then explicit allow:

* Team A cannot call Team B or Team C
* Only Team Câ€™s monitoring services can scrape metrics
* No pod-to-pod cross-tenant traffic

This blocks lateral movement completely.

### âœ… 4. RBAC

Team Aâ€™s users canâ€™t list or modify Team B or Team C resources.

### âœ… 5. Pod Security + Kyverno

Kyverno enforces:

* non-root containers
* no hostPath
* resource limits
* image registry restrictions
* namespace required labels
* automatic network protection

### âœ… 6. Resource Quotas + LimitRanges

Stops noisy-neighbor problems:

* Max CPU/memory per tenant
* Pod count limits
* PVC count caps

This keeps the cluster stable even under load.

---

# ğŸ‘¥ The Three-Team Model

### **Team A â€“ Application Team**

* Gets their own namespace (`team-alpha`)
* Deploys their app through Helm + Argo CD
* Has access only to their workloads
* Subject to security and resource policies

### **Team B â€“ Security & Compliance**

* Owns Kyverno and Policy Reporter
* Manages security policies for the entire cluster
* Does not deploy into Team Aâ€™s namespace
* Enforces Pod Security Standards and safe defaults

### **Team C â€“ Monitoring**

* Runs Prometheus + Grafana
* Can read metrics from all namespaces
* Cannot modify workloads in Team A or Team B
* Provides tenant-aware dashboards

This structure cleanly separates responsibilities while maintaining strong isolation.

---

# ğŸ” Why This Architecture Matters

Most â€œmulti-tenant clusterâ€ examples only talk about:

âœ… namespaces
âœ… RBAC
âŒ but forget GitOps boundaries
âŒ forget network isolation
âŒ forget resource isolation
âŒ forget Pod Security
âŒ forget cross-team visibility rules

This project solves the whole picture.

It demonstrates:

* How tenants can safely deploy their own apps
* How security teams enforce global policies without blocking developers
* How monitoring teams observe workloads without breaking isolation
* How GitOps fits naturally into a multi-tenant model
* How all of this works in an automated, scalable way

---

# ğŸš€ GitOps Workflow (Built for Multi-Tenant Workloads)

1. Developer commits code
2. GitHub Actions runs policy checks
3. Argo CD syncs only within allowed boundaries (Team A namespace only)
4. Kyverno validates security rules
5. Policies and workloads appear in Policy Reporter
6. Prometheus + Grafana show per-tenant dashboards

Every step respects isolation.

---

# ğŸ›¡ï¸ Multi-Tenancy Example: Team A

Hereâ€™s a simplified view of what happens when Team A tries to deploy something insecure:

* Argo CD accepts the change **only if itâ€™s inside Team Aâ€™s AppProject rules**
* Kyverno rejects unsafe pods
* NetworkPolicies prevent cross-namespace traffic
* Grafana only shows Team Aâ€™s metrics
* RBAC prevents Team A from even listing Team Bâ€™s or Team Câ€™s resources

This is real isolation.

---

# ğŸ“ˆ What This Platform Shows

This project demonstrates how to run **one shared Kubernetes cluster** for multiple teams, with:

* Separation of concerns
* Strict isolation
* Automated governance
* Clean GitOps workflows
* Scalable security
* Observable workloads

This is the same model used in large enterprises when managing:

* 50+ dev teams
* regulated industries
* shared CI/CD platforms
* internal platforms with multiple products

---